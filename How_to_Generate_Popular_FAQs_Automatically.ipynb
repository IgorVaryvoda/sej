{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "How to Generate Popular FAQs Automatically.ipynb",
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyNiWvnPNjcgJnxVZMbV7h6f",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hamletbatista/sej/blob/master/How_to_Generate_Popular_FAQs_Automatically.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QLuwHx042c20",
        "colab_type": "text"
      },
      "source": [
        "#How to Generate Popular FAQs Automatically\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YH0RSTEl2fZ_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "url = \"https://sirv.com/blog/increase-shopify-sales/\" #@param {type:\"string\"}\n",
        "selector = \"p\" #@param {type:\"string\"}\n"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7-WXqXFI49B5",
        "colab_type": "text"
      },
      "source": [
        "## Installing the required libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nOOxj_r12uVB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%%capture\n",
        "!pip install requests-html transformers jinja2\n"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XB-1ad9z4UIJ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        },
        "outputId": "7c8c8638-cd9f-43d0-bcf0-633eb8b6da81"
      },
      "source": [
        "!pip show requests-html"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Name: requests-html\n",
            "Version: 0.10.0\n",
            "Summary: HTML Parsing for Humans.\n",
            "Home-page: https://github.com/kennethreitz/requests-html\n",
            "Author: Kenneth Reitz\n",
            "Author-email: me@kennethreitz.org\n",
            "License: MIT\n",
            "Location: /usr/local/lib/python3.6/dist-packages\n",
            "Requires: parse, bs4, requests, pyppeteer, fake-useragent, pyquery, w3lib\n",
            "Required-by: \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JednRVLe4Y-W",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 207
        },
        "outputId": "db159dcc-b841-45dd-bee9-406d7ada6ac3"
      },
      "source": [
        "!pip show transformers"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Name: transformers\n",
            "Version: 3.1.0\n",
            "Summary: State-of-the-art Natural Language Processing for TensorFlow 2.0 and PyTorch\n",
            "Home-page: https://github.com/huggingface/transformers\n",
            "Author: Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Sam Shleifer, Patrick von Platen, Google AI Language Team Authors, Open AI team Authors, Facebook AI Authors, Carnegie Mellon University Authors\n",
            "Author-email: thomas@huggingface.co\n",
            "License: Apache\n",
            "Location: /usr/local/lib/python3.6/dist-packages\n",
            "Requires: filelock, numpy, tqdm, regex, packaging, sacremoses, requests, sentencepiece, dataclasses, tokenizers\n",
            "Required-by: \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MTCMIP2q4zWL",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        },
        "outputId": "94a3d5a3-14e8-44bf-a44d-61b57bf1c262"
      },
      "source": [
        "!pip show nltk"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Name: nltk\n",
            "Version: 3.2.5\n",
            "Summary: Natural Language Toolkit\n",
            "Home-page: http://nltk.org/\n",
            "Author: Steven Bird\n",
            "Author-email: stevenbird1@gmail.com\n",
            "License: Apache License, Version 2.0\n",
            "Location: /usr/local/lib/python3.6/dist-packages\n",
            "Requires: six\n",
            "Required-by: textblob\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oKSBgvVD4289",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        },
        "outputId": "2f8781ca-cd67-4d63-8509-efa331ac500a"
      },
      "source": [
        "!python -m nltk.downloader punkt"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/lib/python3.6/runpy.py:125: RuntimeWarning: 'nltk.downloader' found in sys.modules after import of package 'nltk', but prior to execution of 'nltk.downloader'; this may result in unpredictable behaviour\n",
            "  warn(RuntimeWarning(msg))\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tXUKMxQO45uw",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        },
        "outputId": "fb4b24fd-6a97-4955-8ee8-5d9651458392"
      },
      "source": [
        "!git clone https://github.com/patil-suraj/question_generation.git"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'question_generation'...\n",
            "remote: Enumerating objects: 173, done.\u001b[K\n",
            "remote: Total 173 (delta 0), reused 0 (delta 0), pack-reused 173\u001b[K\n",
            "Receiving objects: 100% (173/173), 264.42 KiB | 4.99 MiB/s, done.\n",
            "Resolving deltas: 100% (90/90), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y8QRsvPj5JnQ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "49150c6b-a715-4bf9-ef47-7b95483ec036"
      },
      "source": [
        "%cd question_generation\n"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/question_generation\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K_C8rlap6hMn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EbxzvBOh6ls7",
        "colab_type": "text"
      },
      "source": [
        "## Fetching the content"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kWyPwzSF6wu9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from requests_html import HTMLSession\n",
        "session = HTMLSession()\n",
        "\n",
        "with session.get(url) as r:\n",
        "\n",
        "    paragraph = r.html.find(selector, first=False)\n",
        "\n",
        "    text = \" \".join([ p.text for p in paragraph])"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hMqTmMD56zKr",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 188
        },
        "outputId": "893cf443-4b3a-4c13-9a26-2ffe5ab0aa08"
      },
      "source": [
        "text"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'Google has an announcement in accounts: starting in September, search queries without a \"significant\" amount of data will no longer show in query reports. Google has quietly slipped an update into their Search Term report data explanations. A vague warning is appearing in the account alerts section:  Clicking on the “Learn More” takes the user to the existing Google support article about Search Terms. At first glance, it looks like the same explanation that’s always been there, but underneath that is the following warning:  The announcement offers no further explanation on what will be considered “significant.” They offered a follow up statement that does little to shed additional light on what the change will involve: In order to maintain our standards of privacy and strengthen our protections around user data, we have made changes to our Search Terms Report to only include terms that a significant number of users searched for. We’re continuing to invest in new and efficient ways to share insights that enable advertisers to make critical business decisions. – Google statement The initial reaction by paid search managers has been one of frustration and skepticism. The immediate concern is for industries that carry high CPCs footing the bill on search queries that are irrelevant.  But there is something deeper at play. This change is one of many that’s leaving managers feeling they have less control over their accounts, and less helpful information from Google.  Some are also pointing out that it wouldn’t be quite the gut punch it seems to be if the aforementioned other changes weren’t occurring. Notably, many have been frustrated with Google’s move years ago to make exact match not exact anymore. The proliferation of keywords that weren’t intended by managers in combination with a lack of visibility, and high CPC environments is creating a different level of angst.  Some PPC managers are offering up examples of where their concern lies: accounts they’ve personally seen waste money which may have never been caught if this new search term rule was at play.  Wil Reynolds also offered a thread giving an example of a bank that was paying for clicks on a query that was actually a rap song:  Some managers see this as an inevitability that just needs to be adapted to as the platform evolves. Many of Google’s features in the past couple years have focused on automation options that have long hinted they want managers to focus more on large-scale account impact versus granularity in pieces like keywords.  As recently as last week, some managers were noticing a difference in the interface buried in the ad creative section. Where it used to have an option for adding creative that could be specified as a Text Ad versus a Responsive Search Ad, the Text Ad option was gone. Google confirmed this is a test, and it not present in every account. Indeed, I am personally able to see it in one account, and not another, though both are owned by the same brand. Here is a view with and without the Text Ad option.  No doubt we will continue to see small changes that could add up to big chances over time for how PPC managers do their jobs. We will continue to follow this developing story.    Get our daily newsletter from SEJ\\'s Founder Loren Baker about the latest news in the industry! Susan is the Paid Media Reporter for Search Engine Journal. A marketing veteran of almost 20 years, she has managed ... [Read full bio] Get our daily newsletter from SEJ\\'s Founder Loren Baker about the latest news in the industry! Get our daily newsletter from SEJ\\'s Founder Loren Baker about the latest news in the industry!'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "40GO9Hdl64Xs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8ZIV9VIf7Bv1",
        "colab_type": "text"
      },
      "source": [
        "## Generating the FAQs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CznYifRJ_ZXY",
        "colab_type": "text"
      },
      "source": [
        "There is a bug at the time of writing this that is being worked on https://github.com/patil-suraj/question_generation/issues/11\n",
        "\n",
        "```---------------------------------------------------------------------------\n",
        "ValueError                                Traceback (most recent call last)\n",
        "<ipython-input-23-fdf5d062d391> in <module>()\n",
        "----> 1 nlp(text)\n",
        "\n",
        "2 frames\n",
        "/content/question_generation/pipelines.py in _prepare_inputs_for_qg_from_answers_hl(self, sents, answers)\n",
        "    140                 answer_text = answer_text.strip()\n",
        "    141 \n",
        "--> 142                 ans_start_idx = sent.index(answer_text)\n",
        "    143 \n",
        "    144                 sent = f\"{sent[:ans_start_idx]} <hl> {answer_text} <hl> {sent[ans_start_idx + len(answer_text): ]}\"\n",
        "\n",
        "ValueError: substring not found\n",
        "```\n",
        "\n",
        "Below is a temporary solution that you can skip if you don't get it"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q_Wt6CKz_4FY",
        "colab_type": "text"
      },
      "source": [
        "## Temporary bug fix\n",
        "\n",
        "Just execute the cell and it will patch the file pipelines.py"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FUoBp58Z7Xl9",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "f363c3d3-9903-49d3-a06a-c7f2f03c8f75"
      },
      "source": [
        "# temporary solution\n",
        "#https://github.com/patil-suraj/question_generation/issues/22\n",
        "\n",
        "%%writefile /content/question_generation/pipelines.py \n",
        "\n",
        "import itertools\n",
        "import logging\n",
        "from typing import Optional, Dict, Union\n",
        "\n",
        "from nltk import sent_tokenize\n",
        "\n",
        "import torch\n",
        "from transformers import(\n",
        "    AutoModelForSeq2SeqLM, \n",
        "    AutoTokenizer,\n",
        "    PreTrainedModel,\n",
        "    PreTrainedTokenizer,\n",
        ")\n",
        "\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "class QGPipeline:\n",
        "    \"\"\"Poor man's QG pipeline\"\"\"\n",
        "    def __init__(\n",
        "        self,\n",
        "        model: PreTrainedModel,\n",
        "        tokenizer: PreTrainedTokenizer,\n",
        "        ans_model: PreTrainedModel,\n",
        "        ans_tokenizer: PreTrainedTokenizer,\n",
        "        qg_format: str,\n",
        "        use_cuda: bool\n",
        "    ):\n",
        "        self.model = model\n",
        "        self.tokenizer = tokenizer\n",
        "\n",
        "        self.ans_model = ans_model\n",
        "        self.ans_tokenizer = ans_tokenizer\n",
        "\n",
        "        self.qg_format = qg_format\n",
        "\n",
        "        self.device = \"cuda\" if torch.cuda.is_available() and use_cuda else \"cpu\"\n",
        "        self.model.to(self.device)\n",
        "\n",
        "        if self.ans_model is not self.model:\n",
        "            self.ans_model.to(self.device)\n",
        "\n",
        "        assert self.model.__class__.__name__ in [\"T5ForConditionalGeneration\", \"BartForConditionalGeneration\"]\n",
        "        \n",
        "        if \"T5ForConditionalGeneration\" in self.model.__class__.__name__:\n",
        "            self.model_type = \"t5\"\n",
        "        else:\n",
        "            self.model_type = \"bart\"\n",
        "\n",
        "    def __call__(self, inputs: str):\n",
        "        inputs = \" \".join(inputs.split())\n",
        "        sents, answers = self._extract_answers(inputs)\n",
        "        flat_answers = list(itertools.chain(*answers))\n",
        "        \n",
        "        if len(flat_answers) == 0:\n",
        "          return []\n",
        "\n",
        "        if self.qg_format == \"prepend\":\n",
        "            qg_examples = self._prepare_inputs_for_qg_from_answers_prepend(inputs, answers)\n",
        "        else:\n",
        "            qg_examples = self._prepare_inputs_for_qg_from_answers_hl(sents, answers)\n",
        "        \n",
        "        qg_inputs = [example['source_text'] for example in qg_examples]\n",
        "        questions = self._generate_questions(qg_inputs)\n",
        "        output = [{'answer': example['answer'], 'question': que} for example, que in zip(qg_examples, questions)]\n",
        "        return output\n",
        "    \n",
        "    def _generate_questions(self, inputs):\n",
        "        inputs = self._tokenize(inputs, padding=True, truncation=True)\n",
        "        \n",
        "        outs = self.model.generate(\n",
        "            input_ids=inputs['input_ids'].to(self.device), \n",
        "            attention_mask=inputs['attention_mask'].to(self.device), \n",
        "            max_length=32,\n",
        "            num_beams=4,\n",
        "        )\n",
        "        \n",
        "        questions = [self.tokenizer.decode(ids, skip_special_tokens=True) for ids in outs]\n",
        "        return questions\n",
        "    \n",
        "    def _extract_answers(self, context):\n",
        "        sents, inputs = self._prepare_inputs_for_ans_extraction(context)\n",
        "        inputs = self._tokenize(inputs, padding=True, truncation=True)\n",
        "\n",
        "        outs = self.ans_model.generate(\n",
        "            input_ids=inputs['input_ids'].to(self.device), \n",
        "            attention_mask=inputs['attention_mask'].to(self.device), \n",
        "            max_length=32,\n",
        "        )\n",
        "        \n",
        "        dec = [self.ans_tokenizer.decode(ids, skip_special_tokens=False) for ids in outs]\n",
        "        answers = [item.split('<sep>') for item in dec]\n",
        "        answers = [i[:-1] for i in answers]\n",
        "        \n",
        "        return sents, answers\n",
        "    \n",
        "    def _tokenize(self,\n",
        "        inputs,\n",
        "        padding=True,\n",
        "        truncation=True,\n",
        "        add_special_tokens=True,\n",
        "        max_length=512\n",
        "    ):\n",
        "        inputs = self.tokenizer.batch_encode_plus(\n",
        "            inputs, \n",
        "            max_length=max_length,\n",
        "            add_special_tokens=add_special_tokens,\n",
        "            truncation=truncation,\n",
        "            padding=\"max_length\" if padding else False,\n",
        "            pad_to_max_length=padding,\n",
        "            return_tensors=\"pt\"\n",
        "        )\n",
        "        return inputs\n",
        "    \n",
        "    def _prepare_inputs_for_ans_extraction(self, text):\n",
        "        sents = sent_tokenize(text)\n",
        "\n",
        "        inputs = []\n",
        "        for i in range(len(sents)):\n",
        "            source_text = \"extract answers:\"\n",
        "            for j, sent in enumerate(sents):\n",
        "                if i == j:\n",
        "                    sent = \"<hl> %s <hl>\" % sent\n",
        "                source_text = \"%s %s\" % (source_text, sent)\n",
        "                source_text = source_text.strip()\n",
        "            \n",
        "            if self.model_type == \"t5\":\n",
        "                source_text = source_text + \" </s>\"\n",
        "            inputs.append(source_text)\n",
        "\n",
        "        return sents, inputs\n",
        "    \n",
        "    def _prepare_inputs_for_qg_from_answers_hl(self, sents, answers):\n",
        "        inputs = []\n",
        "        for i, answer in enumerate(answers):\n",
        "            if len(answer) == 0: continue\n",
        "            sent = sents[i]\n",
        "            for answer_text in answer:\n",
        "                sents_copy = sents[:]\n",
        "                \n",
        "                answer_text = answer_text.strip()\n",
        "                \n",
        "                #ans_start_idx = sent.index(answer_text)\n",
        "                #bypass missmatched Q/As. See https://github.com/patil-suraj/question_generation/issues/22\n",
        "                if answer_text in sent: \n",
        "                  ans_start_idx = sent.index(answer_text) \n",
        "                else: \n",
        "                  continue\n",
        "                \n",
        "                sent = f\"{sent[:ans_start_idx]} <hl> {answer_text} <hl> {sent[ans_start_idx + len(answer_text): ]}\"\n",
        "                sents_copy[i] = sent\n",
        "                \n",
        "                source_text = \" \".join(sents_copy)\n",
        "                source_text = f\"generate question: {source_text}\" \n",
        "                if self.model_type == \"t5\":\n",
        "                    source_text = source_text + \" </s>\"\n",
        "                \n",
        "                inputs.append({\"answer\": answer_text, \"source_text\": source_text})\n",
        "        \n",
        "        return inputs\n",
        "    \n",
        "    def _prepare_inputs_for_qg_from_answers_prepend(self, context, answers):\n",
        "        flat_answers = list(itertools.chain(*answers))\n",
        "        examples = []\n",
        "        for answer in flat_answers:\n",
        "            source_text = f\"answer: {answer} context: {context}\"\n",
        "            if self.model_type == \"t5\":\n",
        "                source_text = source_text + \" </s>\"\n",
        "            \n",
        "            examples.append({\"answer\": answer, \"source_text\": source_text})\n",
        "        return examples\n",
        "\n",
        "    \n",
        "class MultiTaskQAQGPipeline(QGPipeline):\n",
        "    def __init__(self, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "    \n",
        "    def __call__(self, inputs: Union[Dict, str]):\n",
        "        if type(inputs) is str:\n",
        "            # do qg\n",
        "            return super().__call__(inputs)\n",
        "        else:\n",
        "            # do qa\n",
        "            return self._extract_answer(inputs[\"question\"], inputs[\"context\"])\n",
        "    \n",
        "    def _prepare_inputs_for_qa(self, question, context):\n",
        "        source_text = f\"question: {question}  context: {context}\"\n",
        "        if self.model_type == \"t5\":\n",
        "            source_text = source_text + \" </s>\"\n",
        "        return  source_text\n",
        "    \n",
        "    def _extract_answer(self, question, context):\n",
        "        source_text = self._prepare_inputs_for_qa(question, context)\n",
        "        inputs = self._tokenize([source_text], padding=False)\n",
        "    \n",
        "        outs = self.model.generate(\n",
        "            input_ids=inputs['input_ids'].to(self.device), \n",
        "            attention_mask=inputs['attention_mask'].to(self.device), \n",
        "            max_length=16,\n",
        "        )\n",
        "\n",
        "        answer = self.tokenizer.decode(outs[0], skip_special_tokens=True)\n",
        "        return answer\n",
        "\n",
        "\n",
        "class E2EQGPipeline:\n",
        "    def __init__(\n",
        "        self,\n",
        "        model: PreTrainedModel,\n",
        "        tokenizer: PreTrainedTokenizer,\n",
        "        use_cuda: bool\n",
        "    ) :\n",
        "\n",
        "        self.model = model\n",
        "        self.tokenizer = tokenizer\n",
        "\n",
        "        self.device = \"cuda\" if torch.cuda.is_available() and use_cuda else \"cpu\"\n",
        "        self.model.to(self.device)\n",
        "\n",
        "        assert self.model.__class__.__name__ in [\"T5ForConditionalGeneration\", \"BartForConditionalGeneration\"]\n",
        "        \n",
        "        if \"T5ForConditionalGeneration\" in self.model.__class__.__name__:\n",
        "            self.model_type = \"t5\"\n",
        "        else:\n",
        "            self.model_type = \"bart\"\n",
        "        \n",
        "        self.default_generate_kwargs = {\n",
        "            \"max_length\": 256,\n",
        "            \"num_beams\": 4,\n",
        "            \"length_penalty\": 1.5,\n",
        "            \"no_repeat_ngram_size\": 3,\n",
        "            \"early_stopping\": True,\n",
        "        }\n",
        "    \n",
        "    def __call__(self, context: str, **generate_kwargs):\n",
        "        inputs = self._prepare_inputs_for_e2e_qg(context)\n",
        "\n",
        "        # TODO: when overrding default_generate_kwargs all other arguments need to be passsed\n",
        "        # find a better way to do this\n",
        "        if not generate_kwargs:\n",
        "            generate_kwargs = self.default_generate_kwargs\n",
        "        \n",
        "        input_length = inputs[\"input_ids\"].shape[-1]\n",
        "        \n",
        "        # max_length = generate_kwargs.get(\"max_length\", 256)\n",
        "        # if input_length < max_length:\n",
        "        #     logger.warning(\n",
        "        #         \"Your max_length is set to {}, but you input_length is only {}. You might consider decreasing max_length manually, e.g. summarizer('...', max_length=50)\".format(\n",
        "        #             max_length, input_length\n",
        "        #         )\n",
        "        #     )\n",
        "\n",
        "        outs = self.model.generate(\n",
        "            input_ids=inputs['input_ids'].to(self.device), \n",
        "            attention_mask=inputs['attention_mask'].to(self.device),\n",
        "            **generate_kwargs\n",
        "        )\n",
        "\n",
        "        prediction = self.tokenizer.decode(outs[0], skip_special_tokens=True)\n",
        "        questions = prediction.split(\"<sep>\")\n",
        "        questions = [question.strip() for question in questions[:-1]]\n",
        "        return questions\n",
        "    \n",
        "    def _prepare_inputs_for_e2e_qg(self, context):\n",
        "        source_text = f\"generate questions: {context}\"\n",
        "        if self.model_type == \"t5\":\n",
        "            source_text = source_text + \" </s>\"\n",
        "        \n",
        "        inputs = self._tokenize([source_text], padding=False)\n",
        "        return inputs\n",
        "    \n",
        "    def _tokenize(\n",
        "        self,\n",
        "        inputs,\n",
        "        padding=True,\n",
        "        truncation=True,\n",
        "        add_special_tokens=True,\n",
        "        max_length=512\n",
        "    ):\n",
        "        inputs = self.tokenizer.batch_encode_plus(\n",
        "            inputs, \n",
        "            max_length=max_length,\n",
        "            add_special_tokens=add_special_tokens,\n",
        "            truncation=truncation,\n",
        "            padding=\"max_length\" if padding else False,\n",
        "            pad_to_max_length=padding,\n",
        "            return_tensors=\"pt\"\n",
        "        )\n",
        "        return inputs\n",
        "\n",
        "\n",
        "SUPPORTED_TASKS = {\n",
        "    \"question-generation\": {\n",
        "        \"impl\": QGPipeline,\n",
        "        \"default\": {\n",
        "            \"model\": \"valhalla/t5-small-qg-hl\",\n",
        "            \"ans_model\": \"valhalla/t5-small-qa-qg-hl\",\n",
        "        }\n",
        "    },\n",
        "    \"multitask-qa-qg\": {\n",
        "        \"impl\": MultiTaskQAQGPipeline,\n",
        "        \"default\": {\n",
        "            \"model\": \"valhalla/t5-small-qa-qg-hl\",\n",
        "        }\n",
        "    },\n",
        "    \"e2e-qg\": {\n",
        "        \"impl\": E2EQGPipeline,\n",
        "        \"default\": {\n",
        "            \"model\": \"valhalla/t5-small-e2e-qg\",\n",
        "        }\n",
        "    }\n",
        "}\n",
        "\n",
        "def pipeline(\n",
        "    task: str,\n",
        "    model: Optional = None,\n",
        "    tokenizer: Optional[Union[str, PreTrainedTokenizer]] = None,\n",
        "    qg_format: Optional[str] = \"highlight\",\n",
        "    ans_model: Optional = None,\n",
        "    ans_tokenizer: Optional[Union[str, PreTrainedTokenizer]] = None,\n",
        "    use_cuda: Optional[bool] = True,\n",
        "    **kwargs,\n",
        "):\n",
        "    # Retrieve the task\n",
        "    if task not in SUPPORTED_TASKS:\n",
        "        raise KeyError(\"Unknown task {}, available tasks are {}\".format(task, list(SUPPORTED_TASKS.keys())))\n",
        "\n",
        "    targeted_task = SUPPORTED_TASKS[task]\n",
        "    task_class = targeted_task[\"impl\"]\n",
        "\n",
        "    # Use default model/config/tokenizer for the task if no model is provided\n",
        "    if model is None:\n",
        "        model = targeted_task[\"default\"][\"model\"]\n",
        "    \n",
        "    # Try to infer tokenizer from model or config name (if provided as str)\n",
        "    if tokenizer is None:\n",
        "        if isinstance(model, str):\n",
        "            tokenizer = model\n",
        "        else:\n",
        "            # Impossible to guest what is the right tokenizer here\n",
        "            raise Exception(\n",
        "                \"Impossible to guess which tokenizer to use. \"\n",
        "                \"Please provided a PretrainedTokenizer class or a path/identifier to a pretrained tokenizer.\"\n",
        "            )\n",
        "    \n",
        "    # Instantiate tokenizer if needed\n",
        "    if isinstance(tokenizer, (str, tuple)):\n",
        "        if isinstance(tokenizer, tuple):\n",
        "            # For tuple we have (tokenizer name, {kwargs})\n",
        "            tokenizer = AutoTokenizer.from_pretrained(tokenizer[0], **tokenizer[1])\n",
        "        else:\n",
        "            tokenizer = AutoTokenizer.from_pretrained(tokenizer)\n",
        "    \n",
        "    # Instantiate model if needed\n",
        "    if isinstance(model, str):\n",
        "        model = AutoModelForSeq2SeqLM.from_pretrained(model)\n",
        "    \n",
        "    if task == \"question-generation\":\n",
        "        if ans_model is None:\n",
        "            # load default ans model\n",
        "            ans_model = targeted_task[\"default\"][\"ans_model\"]\n",
        "            ans_tokenizer = AutoTokenizer.from_pretrained(ans_model)\n",
        "            ans_model = AutoModelForSeq2SeqLM.from_pretrained(ans_model)\n",
        "        else:\n",
        "            # Try to infer tokenizer from model or config name (if provided as str)\n",
        "            if ans_tokenizer is None:\n",
        "                if isinstance(ans_model, str):\n",
        "                    ans_tokenizer = ans_model\n",
        "                else:\n",
        "                    # Impossible to guest what is the right tokenizer here\n",
        "                    raise Exception(\n",
        "                        \"Impossible to guess which tokenizer to use. \"\n",
        "                        \"Please provided a PretrainedTokenizer class or a path/identifier to a pretrained tokenizer.\"\n",
        "                    )\n",
        "            \n",
        "            # Instantiate tokenizer if needed\n",
        "            if isinstance(ans_tokenizer, (str, tuple)):\n",
        "                if isinstance(ans_tokenizer, tuple):\n",
        "                    # For tuple we have (tokenizer name, {kwargs})\n",
        "                    ans_tokenizer = AutoTokenizer.from_pretrained(ans_tokenizer[0], **ans_tokenizer[1])\n",
        "                else:\n",
        "                    ans_tokenizer = AutoTokenizer.from_pretrained(ans_tokenizer)\n",
        "\n",
        "            if isinstance(ans_model, str):\n",
        "                ans_model = AutoModelForSeq2SeqLM.from_pretrained(ans_model)\n",
        "    \n",
        "    if task == \"e2e-qg\":\n",
        "        return task_class(model=model, tokenizer=tokenizer, use_cuda=use_cuda)\n",
        "    elif task == \"question-generation\":\n",
        "        return task_class(model=model, tokenizer=tokenizer, ans_model=ans_model, ans_tokenizer=ans_tokenizer, qg_format=qg_format, use_cuda=use_cuda)\n",
        "    else:\n",
        "        return task_class(model=model, tokenizer=tokenizer, ans_model=model, ans_tokenizer=tokenizer, qg_format=qg_format, use_cuda=use_cuda)\n",
        "\n"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Overwriting /content/question_generation/pipelines.py\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ujo232gj7EWN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from pipelines import pipeline\n"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "geu8CFFn7I1B",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "nlp = pipeline(\"multitask-qa-qg\")"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VdpsuGzdAH_h",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "faqs = nlp(text)"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "exIdN9M1EHOh",
        "colab_type": "text"
      },
      "source": [
        "Here are the FAQs generated."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XfMbddL6AQjl",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 513
        },
        "outputId": "2009999a-6b1b-4dfc-cfb3-27afefda4828"
      },
      "source": [
        "faqs"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'answer': 'September',\n",
              "  'question': 'When does Google announce that search queries without a \"significant\" amount of data will no longer show in query reports?'},\n",
              " {'answer': 'Google',\n",
              "  'question': 'Who has quietly slipped an update into their Search Term report data explanations?'},\n",
              " {'answer': 'Clicking on the “Learn More” takes the user to the existing Google support article about Search Terms',\n",
              "  'question': 'What is a warning in the account alerts section?'},\n",
              " {'answer': 'advertisers',\n",
              "  'question': 'Who can make critical business decisions?'},\n",
              " {'answer': 'frustration and skepticism',\n",
              "  'question': 'What was the initial reaction by paid search managers?'},\n",
              " {'answer': 'high CPCs',\n",
              "  'question': 'What is the immediate concern for industries that carry footing the bill on search queries that are irrelevant?'},\n",
              " {'answer': 'deeper',\n",
              "  'question': 'What is the immediate concern for industries that carry high CPCs footing the bill on search queries that are irrelevant?'},\n",
              " {'answer': 'less',\n",
              "  'question': 'How much control do managers feel about their accounts?'},\n",
              " {'answer': 'it wouldn’t be quite the gut punch it seems to be if the aforementioned other changes weren’t occurring',\n",
              "  'question': 'What are some of the managers pointing out about the changes made by Google?'},\n",
              " {'answer': 'to make exact match not exact anymore',\n",
              "  'question': \"What did many have been frustrated with Google's move years ago?\"},\n",
              " {'answer': 'angst',\n",
              "  'question': 'The proliferation of keywords that weren’t intended by managers in combination with a lack of visibility, and high CPC environments is creating a different level'},\n",
              " {'answer': 'waste money',\n",
              "  'question': 'What do PPC managers see as a result of a new search term rule?'},\n",
              " {'answer': 'Wil Reynolds',\n",
              "  'question': 'Who offered an example of a bank that was paying for clicks on a query that was actually a rap song?'},\n",
              " {'answer': 'automation',\n",
              "  'question': \"What are some of Google's features in the past couple years focused on?\"}]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z7GubX-cDBsQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eS8PmZSeEMMc",
        "colab_type": "text"
      },
      "source": [
        "## FAQPage schema generation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N0B12hRcEQWE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from jinja2 import Template"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y5Ov6GKzET07",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "faqpage_template=\"\"\"<script type=\"application/ld+json\">\n",
        "    {\n",
        "      \"@context\": \"https://schema.org\",\n",
        "      \"@type\": \"FAQPage\",\n",
        "      \"mainEntity\": [\n",
        "    {% for faq in faqs %}\n",
        "\n",
        "    {\n",
        "        \"@type\": \"Question\",\n",
        "        \"name\": {{faq.question|tojson}},\n",
        "        \"acceptedAnswer\": {\n",
        "          \"@type\": \"Answer\",\n",
        "          \"text\": {{faq.answer|tojson}} \n",
        "          }\n",
        "    }{{ \",\" if not loop.last }}\n",
        "\n",
        "    {% endfor %}\n",
        "    ]\n",
        "    }\n",
        "    </script>\"\"\"\n"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UDingZjYEWqF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "template=Template(faqpage_template)\n",
        "\n",
        "faqpage_output=template.render(faqs=faqs)"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xDjS3AnBEa9H",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "a82ff914-3bee-45f3-bfee-7ce85291338f"
      },
      "source": [
        "print(faqpage_output)"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<script type=\"application/ld+json\">\n",
            "    {\n",
            "      \"@context\": \"https://schema.org\",\n",
            "      \"@type\": \"FAQPage\",\n",
            "      \"mainEntity\": [\n",
            "    \n",
            "\n",
            "    {\n",
            "        \"@type\": \"Question\",\n",
            "        \"name\": \"When does Google announce that search queries without a \\\"significant\\\" amount of data will no longer show in query reports?\",\n",
            "        \"acceptedAnswer\": {\n",
            "          \"@type\": \"Answer\",\n",
            "          \"text\": \"September\" \n",
            "          }\n",
            "    },\n",
            "\n",
            "    \n",
            "\n",
            "    {\n",
            "        \"@type\": \"Question\",\n",
            "        \"name\": \"Who has quietly slipped an update into their Search Term report data explanations?\",\n",
            "        \"acceptedAnswer\": {\n",
            "          \"@type\": \"Answer\",\n",
            "          \"text\": \"Google\" \n",
            "          }\n",
            "    },\n",
            "\n",
            "    \n",
            "\n",
            "    {\n",
            "        \"@type\": \"Question\",\n",
            "        \"name\": \"What is a warning in the account alerts section?\",\n",
            "        \"acceptedAnswer\": {\n",
            "          \"@type\": \"Answer\",\n",
            "          \"text\": \"Clicking on the \\u201cLearn More\\u201d takes the user to the existing Google support article about Search Terms\" \n",
            "          }\n",
            "    },\n",
            "\n",
            "    \n",
            "\n",
            "    {\n",
            "        \"@type\": \"Question\",\n",
            "        \"name\": \"Who can make critical business decisions?\",\n",
            "        \"acceptedAnswer\": {\n",
            "          \"@type\": \"Answer\",\n",
            "          \"text\": \"advertisers\" \n",
            "          }\n",
            "    },\n",
            "\n",
            "    \n",
            "\n",
            "    {\n",
            "        \"@type\": \"Question\",\n",
            "        \"name\": \"What was the initial reaction by paid search managers?\",\n",
            "        \"acceptedAnswer\": {\n",
            "          \"@type\": \"Answer\",\n",
            "          \"text\": \"frustration and skepticism\" \n",
            "          }\n",
            "    },\n",
            "\n",
            "    \n",
            "\n",
            "    {\n",
            "        \"@type\": \"Question\",\n",
            "        \"name\": \"What is the immediate concern for industries that carry footing the bill on search queries that are irrelevant?\",\n",
            "        \"acceptedAnswer\": {\n",
            "          \"@type\": \"Answer\",\n",
            "          \"text\": \"high CPCs\" \n",
            "          }\n",
            "    },\n",
            "\n",
            "    \n",
            "\n",
            "    {\n",
            "        \"@type\": \"Question\",\n",
            "        \"name\": \"What is the immediate concern for industries that carry high CPCs footing the bill on search queries that are irrelevant?\",\n",
            "        \"acceptedAnswer\": {\n",
            "          \"@type\": \"Answer\",\n",
            "          \"text\": \"deeper\" \n",
            "          }\n",
            "    },\n",
            "\n",
            "    \n",
            "\n",
            "    {\n",
            "        \"@type\": \"Question\",\n",
            "        \"name\": \"How much control do managers feel about their accounts?\",\n",
            "        \"acceptedAnswer\": {\n",
            "          \"@type\": \"Answer\",\n",
            "          \"text\": \"less\" \n",
            "          }\n",
            "    },\n",
            "\n",
            "    \n",
            "\n",
            "    {\n",
            "        \"@type\": \"Question\",\n",
            "        \"name\": \"What are some of the managers pointing out about the changes made by Google?\",\n",
            "        \"acceptedAnswer\": {\n",
            "          \"@type\": \"Answer\",\n",
            "          \"text\": \"it wouldn\\u2019t be quite the gut punch it seems to be if the aforementioned other changes weren\\u2019t occurring\" \n",
            "          }\n",
            "    },\n",
            "\n",
            "    \n",
            "\n",
            "    {\n",
            "        \"@type\": \"Question\",\n",
            "        \"name\": \"What did many have been frustrated with Google\\u0027s move years ago?\",\n",
            "        \"acceptedAnswer\": {\n",
            "          \"@type\": \"Answer\",\n",
            "          \"text\": \"to make exact match not exact anymore\" \n",
            "          }\n",
            "    },\n",
            "\n",
            "    \n",
            "\n",
            "    {\n",
            "        \"@type\": \"Question\",\n",
            "        \"name\": \"The proliferation of keywords that weren\\u2019t intended by managers in combination with a lack of visibility, and high CPC environments is creating a different level\",\n",
            "        \"acceptedAnswer\": {\n",
            "          \"@type\": \"Answer\",\n",
            "          \"text\": \"angst\" \n",
            "          }\n",
            "    },\n",
            "\n",
            "    \n",
            "\n",
            "    {\n",
            "        \"@type\": \"Question\",\n",
            "        \"name\": \"What do PPC managers see as a result of a new search term rule?\",\n",
            "        \"acceptedAnswer\": {\n",
            "          \"@type\": \"Answer\",\n",
            "          \"text\": \"waste money\" \n",
            "          }\n",
            "    },\n",
            "\n",
            "    \n",
            "\n",
            "    {\n",
            "        \"@type\": \"Question\",\n",
            "        \"name\": \"Who offered an example of a bank that was paying for clicks on a query that was actually a rap song?\",\n",
            "        \"acceptedAnswer\": {\n",
            "          \"@type\": \"Answer\",\n",
            "          \"text\": \"Wil Reynolds\" \n",
            "          }\n",
            "    },\n",
            "\n",
            "    \n",
            "\n",
            "    {\n",
            "        \"@type\": \"Question\",\n",
            "        \"name\": \"What are some of Google\\u0027s features in the past couple years focused on?\",\n",
            "        \"acceptedAnswer\": {\n",
            "          \"@type\": \"Answer\",\n",
            "          \"text\": \"automation\" \n",
            "          }\n",
            "    }\n",
            "\n",
            "    \n",
            "    ]\n",
            "    }\n",
            "    </script>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PUnAwdPWH1tN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qrR39mWEIUIP",
        "colab_type": "text"
      },
      "source": [
        "## FAQ HTML  generation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YrpWIwBYIgGn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "faqpage_template=\"\"\" <div id=\"FAQTab\"> \n",
        "{% for faq in faqs %} \n",
        "<div id=\"Question\"> {{faq.question}} </div> \n",
        "<div id=\"Answer\"> <strong>{{faq.answer}} </strong></div>\n",
        "{% endfor %}  \n",
        "</div> \"\"\""
      ],
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9G9txwb6IpAf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "template=Template(faqpage_template)\n",
        "\n",
        "faqpage_output=template.render(faqs=faqs)"
      ],
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qMHQEqByIx9R",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 802
        },
        "outputId": "dd28b818-6401-4790-aab3-0307e05182af"
      },
      "source": [
        "print(faqpage_output)"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            " <div id=\"FAQTab\"> \n",
            " \n",
            "<div id=\"Question\"> When does Google announce that search queries without a \"significant\" amount of data will no longer show in query reports? </div> \n",
            "<div id=\"Answer\"> <strong>September </strong></div>\n",
            " \n",
            "<div id=\"Question\"> Who has quietly slipped an update into their Search Term report data explanations? </div> \n",
            "<div id=\"Answer\"> <strong>Google </strong></div>\n",
            " \n",
            "<div id=\"Question\"> What is a warning in the account alerts section? </div> \n",
            "<div id=\"Answer\"> <strong>Clicking on the “Learn More” takes the user to the existing Google support article about Search Terms </strong></div>\n",
            " \n",
            "<div id=\"Question\"> Who can make critical business decisions? </div> \n",
            "<div id=\"Answer\"> <strong>advertisers </strong></div>\n",
            " \n",
            "<div id=\"Question\"> What was the initial reaction by paid search managers? </div> \n",
            "<div id=\"Answer\"> <strong>frustration and skepticism </strong></div>\n",
            " \n",
            "<div id=\"Question\"> What is the immediate concern for industries that carry footing the bill on search queries that are irrelevant? </div> \n",
            "<div id=\"Answer\"> <strong>high CPCs </strong></div>\n",
            " \n",
            "<div id=\"Question\"> What is the immediate concern for industries that carry high CPCs footing the bill on search queries that are irrelevant? </div> \n",
            "<div id=\"Answer\"> <strong>deeper </strong></div>\n",
            " \n",
            "<div id=\"Question\"> How much control do managers feel about their accounts? </div> \n",
            "<div id=\"Answer\"> <strong>less </strong></div>\n",
            " \n",
            "<div id=\"Question\"> What are some of the managers pointing out about the changes made by Google? </div> \n",
            "<div id=\"Answer\"> <strong>it wouldn’t be quite the gut punch it seems to be if the aforementioned other changes weren’t occurring </strong></div>\n",
            " \n",
            "<div id=\"Question\"> What did many have been frustrated with Google's move years ago? </div> \n",
            "<div id=\"Answer\"> <strong>to make exact match not exact anymore </strong></div>\n",
            " \n",
            "<div id=\"Question\"> The proliferation of keywords that weren’t intended by managers in combination with a lack of visibility, and high CPC environments is creating a different level </div> \n",
            "<div id=\"Answer\"> <strong>angst </strong></div>\n",
            " \n",
            "<div id=\"Question\"> What do PPC managers see as a result of a new search term rule? </div> \n",
            "<div id=\"Answer\"> <strong>waste money </strong></div>\n",
            " \n",
            "<div id=\"Question\"> Who offered an example of a bank that was paying for clicks on a query that was actually a rap song? </div> \n",
            "<div id=\"Answer\"> <strong>Wil Reynolds </strong></div>\n",
            " \n",
            "<div id=\"Question\"> What are some of Google's features in the past couple years focused on? </div> \n",
            "<div id=\"Answer\"> <strong>automation </strong></div>\n",
            "  \n",
            "</div> \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oiXNgIX9I0Xx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}
